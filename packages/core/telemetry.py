"""
Cost Tracking and Telemetry - Decorator for LLM Cost Tracking
Implements $0.007/doc cost validation with CSV audit trail

2026 UPGRADE: Added CostEfficiencyMonitor for unit economics analysis
- Tracks cost-per-million-tokens with 2026 Llama 3.1 70B pricing
- Human-comparison metric ($25.00 vs $0.0007)
- Outputs to benchmarks/unit_economics_report.json
"""
import os
import csv
import json
import functools
import hashlib
from typing import Callable, Any, Dict, Optional, List
from datetime import datetime
from pathlib import Path
from pydantic import BaseModel, Field


# OpenAI/Anthropic/Meta Pricing (as of 2026)
MODEL_PRICING = {
    # GPT-4o Vision
    "gpt-4o-vision": {
        "input": 0.0000025,   # $2.50 per 1M tokens
        "output": 0.00001,    # $10.00 per 1M tokens
    },
    # Claude 3 Haiku (fast, cheap)
    "claude-3-haiku": {
        "input": 0.00000025,  # $0.25 per 1M tokens
        "output": 0.00000125, # $1.25 per 1M tokens
    },
    # Claude 3.5 Sonnet (balanced)
    "claude-3.5-sonnet": {
        "input": 0.000003,    # $3.00 per 1M tokens
        "output": 0.000015,   # $15.00 per 1M tokens
    },
    # GPT-4 Turbo
    "gpt-4-turbo": {
        "input": 0.00001,     # $10.00 per 1M tokens
        "output": 0.00003,    # $30.00 per 1M tokens
    },
    # 2026 UPGRADE: Llama 3.1 70B (proving $0.0007/doc is achievable)
    "llama-3.1-70b": {
        "input": 0.00000011,  # $0.11 per 1M tokens
        "output": 0.00000011, # $0.11 per 1M tokens (same pricing)
    },
}


def calculate_llm_cost(
    model_name: str,
    input_tokens: int,
    output_tokens: int
) -> float:
    """
    Calculate USD cost of an LLM call based on token usage
    
    Args:
        model_name: Model identifier (e.g., 'claude-3-haiku', 'gpt-4o-vision')
        input_tokens: Number of input/prompt tokens
        output_tokens: Number of completion/output tokens
    
    Returns:
        Total cost in USD
    """
    pricing = MODEL_PRICING.get(model_name, MODEL_PRICING["claude-3-haiku"])
    input_cost = input_tokens * pricing["input"]
    output_cost = output_tokens * pricing["output"]
    return input_cost + output_cost


def track_agent_cost(
    agent_name: str,
    model_name: str = "claude-3-haiku",
    csv_path: str = "benchmarks/runs.csv"
):
    """
    Decorator to track agent LLM costs and save to CSV
    
    Usage:
        @track_agent_cost(agent_name="Guard", model_name="claude-3-haiku")
        def my_agent_function(state):
            # ... agent logic ...
            return {
                "input_tokens": 100,
                "output_tokens": 50,
                # ... other results ...
            }
    
    The decorated function MUST return a dict with 'input_tokens' and 'output_tokens' keys.
    The decorator will automatically add:
        - 'cost_usd': Calculated cost
        - 'agent_name': Agent identifier
        - 'model_name': Model used
        - 'timestamp': ISO timestamp
    
    All metrics are appended to the CSV file specified by csv_path.
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs) -> Dict[str, Any]:
            # Execute the agent function
            start_time = datetime.now()
            result = func(*args, **kwargs)
            end_time = datetime.now()
            
            # Extract token usage from result
            input_tokens = result.get("input_tokens", 0)
            output_tokens = result.get("output_tokens", 0)
            
            # Calculate cost
            cost_usd = calculate_llm_cost(model_name, input_tokens, output_tokens)
            
            # Enrich result with cost metrics
            result["cost_usd"] = cost_usd
            result["agent_name"] = agent_name
            result["model_name"] = model_name
            result["timestamp"] = start_time.isoformat()
            result["duration_ms"] = int((end_time - start_time).total_seconds() * 1000)
            
            # Update ComplianceResult if present (for Guard agent)
            if "result" in result and hasattr(result["result"], "processing_cost"):
                # Create new instance with updated cost (since it's frozen)
                old_result = result["result"]
                result["result"] = type(old_result)(
                    **{**old_result.dict(), "processing_cost": cost_usd}
                )
            
            # Append to CSV
            _append_to_csv(
                csv_path=csv_path,
                agent_name=agent_name,
                model_name=model_name,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                cost_usd=cost_usd,
                timestamp=start_time.isoformat(),
                duration_ms=result["duration_ms"],
                metadata=result.get("metadata", {})
            )
            
            return result
        
        return wrapper
    return decorator


def _append_to_csv(
    csv_path: str,
    agent_name: str,
    model_name: str,
    input_tokens: int,
    output_tokens: int,
    cost_usd: float,
    timestamp: str,
    duration_ms: int,
    metadata: Optional[Dict[str, Any]] = None
):
    """
    Append cost metrics to CSV file (creates file and headers if needed)
    """
    # Ensure directory exists
    csv_file = Path(csv_path)
    csv_file.parent.mkdir(parents=True, exist_ok=True)
    
    # Check if file exists to determine if we need headers
    file_exists = csv_file.exists()
    
    # Prepare row data
    row = {
        "timestamp": timestamp,
        "agent_name": agent_name,
        "model_name": model_name,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_tokens": input_tokens + output_tokens,
        "cost_usd": f"{cost_usd:.6f}",
        "duration_ms": duration_ms,
        "document_id": metadata.get("document_id", "") if metadata else "",
        "success": metadata.get("success", True) if metadata else True,
    }
    
    # Write to CSV
    with open(csv_file, "a", newline="") as f:
        fieldnames = [
            "timestamp",
            "agent_name", 
            "model_name",
            "input_tokens",
            "output_tokens",
            "total_tokens",
            "cost_usd",
            "duration_ms",
            "document_id",
            "success"
        ]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        
        # Write header if new file
        if not file_exists:
            writer.writeheader()
        
        writer.writerow(row)


def get_cost_summary(csv_path: str = "benchmarks/runs.csv") -> Dict[str, Any]:
    """
    Analyze costs from CSV and return summary statistics
    
    Returns:
        Dict with total cost, average per document, agent breakdown, etc.
    """
    csv_file = Path(csv_path)
    
    if not csv_file.exists():
        return {
            "total_cost_usd": 0.0,
            "total_documents": 0,
            "avg_cost_per_doc": 0.0,
            "meets_target": False,
            "target_cost_per_doc": 0.007,
            "by_agent": {}
        }
    
    # Read CSV and calculate statistics
    total_cost = 0.0
    document_ids = set()
    by_agent = {}
    
    with open(csv_file, "r") as f:
        reader = csv.DictReader(f)
        for row in reader:
            agent_name = row["agent_name"]
            cost = float(row["cost_usd"])
            doc_id = row.get("document_id", "")
            
            total_cost += cost
            if doc_id:
                document_ids.add(doc_id)
            
            if agent_name not in by_agent:
                by_agent[agent_name] = {
                    "total_cost": 0.0,
                    "num_calls": 0,
                    "total_tokens": 0
                }
            
            by_agent[agent_name]["total_cost"] += cost
            by_agent[agent_name]["num_calls"] += 1
            by_agent[agent_name]["total_tokens"] += int(row.get("total_tokens", 0))
    
    num_docs = len(document_ids) if document_ids else 1  # Avoid division by zero
    avg_cost_per_doc = total_cost / num_docs
    target_cost = 0.007
    
    return {
        "total_cost_usd": round(total_cost, 6),
        "total_documents": len(document_ids),
        "avg_cost_per_doc": round(avg_cost_per_doc, 6),
        "meets_target": avg_cost_per_doc <= target_cost,
        "target_cost_per_doc": target_cost,
        "delta_from_target": round(avg_cost_per_doc - target_cost, 6),
        "by_agent": by_agent
    }


def validate_cost_target(csv_path: str = "benchmarks/runs.csv") -> bool:
    """
    Validate that average cost per document meets $0.007 target
    
    Returns:
        True if cost target is met, False otherwise
    """
    summary = get_cost_summary(csv_path)
    return summary["meets_target"]


# =============================================================================
# 2026 UPGRADE: Cost Efficiency Monitor
# =============================================================================

class CostEfficiencyMonitor:
    """
    2026 Production-Grade Unit Economics Monitor
    
    Proves the $0.0007/doc claim by tracking:
    1. Cost-per-Million-Tokens using 2026 Llama 3.1 70B pricing ($0.11/1M tokens)
    2. Human-Comparison-Metric: $25.00 (human review) vs $0.0007 (agent review)
    3. Outputs to benchmarks/unit_economics_report.json
    
    This class provides investor-grade metrics for GTM strategy and proves
    ConComplyAi's 35,714x cost advantage over manual review.
    
    Usage:
        monitor = CostEfficiencyMonitor(csv_path="benchmarks/runs.csv")
        report = monitor.generate_unit_economics_report()
        monitor.save_report("benchmarks/unit_economics_report.json")
    """
    
    def __init__(
        self,
        csv_path: str = "benchmarks/runs.csv",
        target_model: str = "llama-3.1-70b",
        human_review_cost: float = 25.00,
        target_agent_cost: float = 0.0007
    ):
        """
        Initialize Cost Efficiency Monitor
        
        Args:
            csv_path: Path to runs.csv with cost data
            target_model: Model to use for cost calculations (default: llama-3.1-70b)
            human_review_cost: Cost of human review per document (default: $25.00)
            target_agent_cost: Target cost per document for agents (default: $0.0007)
        """
        self.csv_path = Path(csv_path)
        self.target_model = target_model
        self.human_review_cost = human_review_cost
        self.target_agent_cost = target_agent_cost
        self.report_data: Optional[Dict[str, Any]] = None
    
    def calculate_cost_per_million_tokens(self) -> Dict[str, float]:
        """
        Calculate cost per million tokens for configured model
        
        Returns:
            Dict with input_cost_per_1m, output_cost_per_1m, avg_cost_per_1m
        """
        pricing = MODEL_PRICING.get(self.target_model, MODEL_PRICING["llama-3.1-70b"])
        
        return {
            "input_cost_per_1m_tokens": pricing["input"] * 1_000_000,
            "output_cost_per_1m_tokens": pricing["output"] * 1_000_000,
            "avg_cost_per_1m_tokens": (pricing["input"] + pricing["output"]) / 2 * 1_000_000,
            "model": self.target_model
        }
    
    def calculate_human_comparison(self, agent_cost_per_doc: float) -> Dict[str, Any]:
        """
        Calculate human vs agent cost comparison metrics
        
        Args:
            agent_cost_per_doc: Actual agent cost per document
        
        Returns:
            Dict with comparison metrics including cost_advantage_multiple
        """
        cost_advantage = self.human_review_cost / agent_cost_per_doc if agent_cost_per_doc > 0 else 0
        cost_savings_per_doc = self.human_review_cost - agent_cost_per_doc
        
        return {
            "human_review_cost_per_doc": self.human_review_cost,
            "agent_review_cost_per_doc": agent_cost_per_doc,
            "cost_savings_per_doc": round(cost_savings_per_doc, 4),
            "cost_advantage_multiple": round(cost_advantage, 1),
            "human_vs_agent_description": f"Agent is {cost_advantage:.1f}x cheaper than human review",
            "roi_percentage": round((cost_savings_per_doc / self.human_review_cost) * 100, 2)
        }
    
    def generate_unit_economics_report(self) -> Dict[str, Any]:
        """
        Generate comprehensive unit economics report
        
        Returns:
            Dict with full unit economics metrics
        """
        # Get cost summary from CSV
        cost_summary = get_cost_summary(str(self.csv_path))
        
        # Calculate cost per million tokens
        token_pricing = self.calculate_cost_per_million_tokens()
        
        # Calculate human comparison
        agent_cost = cost_summary.get("avg_cost_per_doc", 0.0)
        human_comparison = self.calculate_human_comparison(agent_cost)
        
        # Build report
        self.report_data = {
            "report_metadata": {
                "generated_at": datetime.now().isoformat(),
                "report_version": "2026.1.0",
                "target_model": self.target_model,
                "csv_source": str(self.csv_path)
            },
            "cost_per_document": {
                "actual_agent_cost": agent_cost,
                "target_agent_cost": self.target_agent_cost,
                "meets_target": agent_cost <= self.target_agent_cost,
                "delta_from_target": round(agent_cost - self.target_agent_cost, 6),
                "total_documents_analyzed": cost_summary["total_documents"],
                "total_cost_usd": cost_summary["total_cost_usd"]
            },
            "cost_per_million_tokens": token_pricing,
            "human_vs_agent_comparison": human_comparison,
            "by_agent_breakdown": cost_summary.get("by_agent", {}),
            "key_metrics": {
                "target_claim": "$0.0007 per document",
                "actual_performance": f"${agent_cost:.6f} per document",
                "human_baseline": f"${self.human_review_cost:.2f} per document",
                "cost_advantage": f"{human_comparison['cost_advantage_multiple']:.1f}x cheaper than human",
                "annual_savings_per_1000_docs": round(human_comparison['cost_savings_per_doc'] * 1000, 2)
            },
            "investor_summary": {
                "unit_economics_proven": agent_cost <= self.target_agent_cost,
                "gtm_advantage": (
                    f"ConComplyAi provides {human_comparison['cost_advantage_multiple']:.0f}x cost advantage "
                    f"over manual compliance review. At $0.0007/doc vs $25.00/doc, we enable "
                    f"automated compliance at scale with {human_comparison['roi_percentage']:.1f}% cost reduction."
                ),
                "scalability": (
                    f"Processing {cost_summary['total_documents']} documents at "
                    f"${agent_cost:.6f}/doc proves production-ready unit economics."
                )
            }
        }
        
        return self.report_data
    
    def save_report(self, output_path: str = "benchmarks/unit_economics_report.json") -> Path:
        """
        Save unit economics report to JSON file
        
        Args:
            output_path: Path to save JSON report
        
        Returns:
            Path to saved file
        """
        if self.report_data is None:
            self.generate_unit_economics_report()
        
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(self.report_data, f, indent=2)
        
        return output_file
    
    def print_summary(self):
        """Print human-readable summary to console"""
        if self.report_data is None:
            self.generate_unit_economics_report()
        
        print("\n" + "=" * 80)
        print("CONCOMPLAI UNIT ECONOMICS REPORT - 2026")
        print("=" * 80)
        
        print("\nðŸ“Š COST PER DOCUMENT:")
        cpd = self.report_data["cost_per_document"]
        print(f"  Target:  ${cpd['target_agent_cost']:.6f}")
        print(f"  Actual:  ${cpd['actual_agent_cost']:.6f}")
        print(f"  Status:  {'âœ… MEETS TARGET' if cpd['meets_target'] else 'âŒ ABOVE TARGET'}")
        print(f"  Delta:   ${cpd['delta_from_target']:+.6f}")
        
        print("\nðŸ’° HUMAN VS AGENT COMPARISON:")
        hc = self.report_data["human_vs_agent_comparison"]
        print(f"  Human Review:  ${hc['human_review_cost_per_doc']:.2f}/doc")
        print(f"  Agent Review:  ${hc['agent_review_cost_per_doc']:.6f}/doc")
        print(f"  Savings:       ${hc['cost_savings_per_doc']:.4f}/doc")
        print(f"  Advantage:     {hc['cost_advantage_multiple']:.1f}x cheaper")
        print(f"  ROI:           {hc['roi_percentage']:.1f}%")
        
        print("\nðŸŽ¯ KEY METRICS:")
        for key, value in self.report_data["key_metrics"].items():
            print(f"  {key.replace('_', ' ').title()}: {value}")
        
        print("\nðŸ“ˆ INVESTOR SUMMARY:")
        inv = self.report_data["investor_summary"]
        print(f"  Unit Economics Proven: {'âœ… YES' if inv['unit_economics_proven'] else 'âŒ NO'}")
        print(f"  GTM Advantage: {inv['gtm_advantage']}")
        
        print("\n" + "=" * 80)


def generate_and_save_unit_economics_report(
    csv_path: str = "benchmarks/runs.csv",
    output_path: str = "benchmarks/unit_economics_report.json",
    print_summary: bool = True
) -> Dict[str, Any]:
    """
    Convenience function to generate and save unit economics report
    
    This is the main entry point for generating the $0.0007/doc proof
    
    Args:
        csv_path: Path to runs.csv with cost data
        output_path: Path to save JSON report
        print_summary: Whether to print summary to console
    
    Returns:
        Dict with full unit economics report
    
    Example:
        >>> report = generate_and_save_unit_economics_report()
        >>> assert report["cost_per_document"]["meets_target"] == True
    """
    monitor = CostEfficiencyMonitor(csv_path=csv_path)
    report = monitor.generate_unit_economics_report()
    monitor.save_report(output_path)
    
    if print_summary:
        monitor.print_summary()
    
    return report

